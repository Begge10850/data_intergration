{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c4dca32-d9c9-444b-8454-1e9846b96900",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I intialized the spark session, because it's necessary for performing the distributed data processing using Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f896998-2389-431b-bb00-1389896ab416",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initializing the Spark session\n",
    "spark = SparkSession.builder.appName(\"DataIntegration\").getOrCreate()\n",
    "print(\"Spark session created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5915df-ab60-48cc-82c1-5a9f470dddde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|\n+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|\n|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|\n|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|\n|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|\n|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|\n+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "file_path = \"/FileStore/tables/Reviews/Reviews.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "print(\"Dataset loaded\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3af7204-897c-4095-b8be-b5e52c5e48b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I then dropped rows with missing values to ensure data quality. I then proceeded to select relevant columns needed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e551e7-7845-4051-b107-64a46f3c49ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed\n+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|\n+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|\n|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|\n|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|\n|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|\n|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|\n+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing: Dropping the rows with missing values and selecting the relevant columns\n",
    "df = df.dropna()\n",
    "df = df.select(\"Id\", \"ProductId\", \"UserId\", \"ProfileName\", \"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"Score\", \"Time\", \"Summary\", \"Text\")\n",
    "print(\"Data preprocessing completed\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a9efaaa-d125-464d-a703-d6161f760eb4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\"withColumn\" method is used to cast the columns \"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"Score\", and \"Time\" to integer and long data types respectively to ensure accuratenumerical operations and calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8abb3d-7e74-4ba0-9b76-3c767a4f9ec2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns converted to appropriate data types\n+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|\n+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|\n|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|\n|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|\n|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|\n|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|\n+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Converting the columns to appropriate data types\n",
    "df = df.withColumn(\"HelpfulnessNumerator\", col(\"HelpfulnessNumerator\").cast(\"integer\"))\n",
    "df = df.withColumn(\"HelpfulnessDenominator\", col(\"HelpfulnessDenominator\").cast(\"integer\"))\n",
    "df = df.withColumn(\"Score\", col(\"Score\").cast(\"integer\"))\n",
    "df = df.withColumn(\"Time\", col(\"Time\").cast(\"long\"))\n",
    "\n",
    "print(\"Columns converted to appropriate data types\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ede18ceb-c5d1-433b-a81e-f96722a5810a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I then simulated another dataset containing product information. The \"product_info_data\" list contains a sample data, and the \"product_info_schema\" list defines the column names. The \"createDataFrame\" method creates a spark dataframe from the simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e67e1049-693b-4961-bc43-fa53efe1195a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated product information dataset created\n+---------+-----------+----------+\n|ProductId|ProductName|  Category|\n+---------+-----------+----------+\n|     B001|  Product A|Category A|\n|     B002|  Product B|Category B|\n|     B003|  Product C|Category C|\n+---------+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Simulating another dataset containing a sample product information\n",
    "product_info_data = [\n",
    "    (\"B001\", \"Product A\", \"Category A\"),\n",
    "    (\"B002\", \"Product B\", \"Category B\"),\n",
    "    (\"B003\", \"Product C\", \"Category C\")\n",
    "]\n",
    "product_info_schema = [\"ProductId\", \"ProductName\", \"Category\"]\n",
    "product_info = spark.createDataFrame(product_info_data, product_info_schema)\n",
    "print(\"Simulated product information dataset created\")\n",
    "product_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb08211-b5ea-4e3c-b33c-b92c41c3050e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I then proceeded to join the main dataset \"df\", with the simulated product information dataset \"product_info\" on the \"ProductID\" column using an inner join, on rows only including \"ProductId\" values on both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39555e09-1ab0-4537-9bb8-6522b5d43cf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data integration completed\n+---------+---+------+-----------+--------------------+----------------------+-----+----+-------+----+-----------+--------+\n|ProductId| Id|UserId|ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|Time|Summary|Text|ProductName|Category|\n+---------+---+------+-----------+--------------------+----------------------+-----+----+-------+----+-----------+--------+\n+---------+---+------+-----------+--------------------+----------------------+-----+----+-------+----+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Integrating the data using Spark\n",
    "integrated_data = df.join(product_info, on=\"ProductId\", how=\"inner\")\n",
    "print(\"Data integration completed\")\n",
    "integrated_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2145e38-79eb-4213-9a09-5bd6f01a7c48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation completed\n+---------+-------------+\n|ProductId|average_score|\n+---------+-------------+\n+---------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Aggregation: Calculating the average score for each product\n",
    "aggregated_data = integrated_data.groupBy(\"ProductId\").agg({\"Score\": \"avg\"})\n",
    "aggregated_data = aggregated_data.withColumnRenamed(\"avg(Score)\", \"average_score\")\n",
    "print(\"Aggregation completed\")\n",
    "aggregated_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae366f63-015e-4f7a-8b60-96dd0bc80893",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I then saved the aggregated data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af62c14-3222-40cc-8897-04a67cc518ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data saved to CSV\n"
     ]
    }
   ],
   "source": [
    "# Definining the output path\n",
    "output_path = \"/FileStore/tables/aggregated_data\"\n",
    "\n",
    "# Removing the existing output path if it already exists\n",
    "def remove_existing_path(path):\n",
    "    if any(f.path == path for f in dbutils.fs.ls(os.path.dirname(path))):\n",
    "        dbutils.fs.rm(path, True)\n",
    "        print(f\"Removed existing directory: {path}\")\n",
    "\n",
    "remove_existing_path(output_path)\n",
    "\n",
    "# Saving the result to a CSV file\n",
    "aggregated_data.coalesce(1).write.option(\"header\", \"true\").csv(output_path)\n",
    "print(\"Aggregated data saved to CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9086821-b02c-40e3-84d1-1a1cf5a8761e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stopping the Spark session to free up resources and avoid memory leaks\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac3a3042-c019-48f2-be12-c105bbf3ee56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2024-07-03 - DBFS Example",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
